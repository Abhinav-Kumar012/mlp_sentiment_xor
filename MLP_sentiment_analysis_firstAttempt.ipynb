{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))            #LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>732</td>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#ScienceFairWinner #HighSchoolScience</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>733</td>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#SurpriseCelebration #HighSchoolFriendship</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>734</td>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#CommunityGiving #HighSchoolPhilanthropy</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>735</td>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CulturalCelebration #HighSchoolUnity</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>732</td>\n",
       "      <td>736</td>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#VirtualEntertainment #HighSchoolPositivity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0               0           0   \n",
       "1               1           1   \n",
       "2               2           2   \n",
       "3               3           3   \n",
       "4               4           4   \n",
       "..            ...         ...   \n",
       "727           728         732   \n",
       "728           729         733   \n",
       "729           730         734   \n",
       "730           731         735   \n",
       "731           732         736   \n",
       "\n",
       "                                                  Text    Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1     Traffic was terrible this morning.           ...   Negative     \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3     Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "..                                                 ...          ...   \n",
       "727  Collaborating on a science project that receiv...       Happy    \n",
       "728  Attending a surprise birthday party organized ...       Happy    \n",
       "729  Successfully fundraising for a school charity ...       Happy    \n",
       "730  Participating in a multicultural festival, cel...       Happy    \n",
       "731  Organizing a virtual talent show during challe...       Happy    \n",
       "\n",
       "               Timestamp                                   User     Platform  \\\n",
       "0    2023-01-15 12:30:00                          User123          Twitter     \n",
       "1    2023-01-15 08:45:00                          CommuterX        Twitter     \n",
       "2    2023-01-15 15:45:00                          FitnessFan      Instagram    \n",
       "3    2023-01-15 18:20:00                          AdventureX       Facebook    \n",
       "4    2023-01-15 19:55:00                          ChefCook        Instagram    \n",
       "..                   ...                                    ...          ...   \n",
       "727  2017-08-18 18:20:00       ScienceProjectSuccessHighSchool     Facebook    \n",
       "728  2018-06-22 14:15:00            BirthdayPartyJoyHighSchool    Instagram    \n",
       "729  2019-04-05 17:30:00   CharityFundraisingTriumphHighSchool      Twitter    \n",
       "730  2020-02-29 20:45:00    MulticulturalFestivalJoyHighSchool     Facebook    \n",
       "731  2020-11-15 15:15:00    VirtualTalentShowSuccessHighSchool    Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  \\\n",
       "0        #Nature #Park                                  15.0   30.0   \n",
       "1        #Traffic #Morning                               5.0   10.0   \n",
       "2        #Fitness #Workout                              20.0   40.0   \n",
       "3        #Travel #Adventure                              8.0   15.0   \n",
       "4        #Cooking #Food                                 12.0   25.0   \n",
       "..                                             ...       ...    ...   \n",
       "727         #ScienceFairWinner #HighSchoolScience       20.0   39.0   \n",
       "728    #SurpriseCelebration #HighSchoolFriendship       25.0   48.0   \n",
       "729      #CommunityGiving #HighSchoolPhilanthropy       22.0   42.0   \n",
       "730         #CulturalCelebration #HighSchoolUnity       21.0   43.0   \n",
       "731   #VirtualEntertainment #HighSchoolPositivity       24.0   47.0   \n",
       "\n",
       "          Country  Year  Month  Day  Hour  \n",
       "0       USA        2023      1   15    12  \n",
       "1       Canada     2023      1   15     8  \n",
       "2     USA          2023      1   15    15  \n",
       "3       UK         2023      1   15    18  \n",
       "4      Australia   2023      1   15    19  \n",
       "..            ...   ...    ...  ...   ...  \n",
       "727            UK  2017      8   18    18  \n",
       "728           USA  2018      6   22    14  \n",
       "729        Canada  2019      4    5    17  \n",
       "730            UK  2020      2   29    20  \n",
       "731           USA  2020     11   15    15  \n",
       "\n",
       "[732 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"./datasets/sentimentdataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    #text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/ywg60v213710yk8053f7h4k00000gn/T/ipykernel_77104/1905376011.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Sentiment\"] = df[\"Sentiment\"].str.strip()\n",
      "/var/folders/mz/ywg60v213710yk8053f7h4k00000gn/T/ipykernel_77104/1905376011.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>enjoying beautiful day park!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>traffic terrible morning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>finished amazing workout! ðŸ’ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excited upcoming weekend getaway!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>trying new recipe dinner tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>collaborating science project received recogni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>attending surprise birthday party organized fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>successfully fundraising school charity initia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>participating multicultural festival, celebrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>organizing virtual talent show challenging tim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...  Positive   \n",
       "1     Traffic was terrible this morning.           ...  Negative   \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...  Positive   \n",
       "3     Excited about the upcoming weekend getaway!  ...  Positive   \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral   \n",
       "..                                                 ...       ...   \n",
       "727  Collaborating on a science project that receiv...     Happy   \n",
       "728  Attending a surprise birthday party organized ...     Happy   \n",
       "729  Successfully fundraising for a school charity ...     Happy   \n",
       "730  Participating in a multicultural festival, cel...     Happy   \n",
       "731  Organizing a virtual talent show during challe...     Happy   \n",
       "\n",
       "                                         ProcessedText  \n",
       "0                         enjoying beautiful day park!  \n",
       "1                            traffic terrible morning.  \n",
       "2                          finished amazing workout! ðŸ’ª  \n",
       "3                    excited upcoming weekend getaway!  \n",
       "4                    trying new recipe dinner tonight.  \n",
       "..                                                 ...  \n",
       "727  collaborating science project received recogni...  \n",
       "728  attending surprise birthday party organized fr...  \n",
       "729  successfully fundraising school charity initia...  \n",
       "730  participating multicultural festival, celebrat...  \n",
       "731  organizing virtual talent show challenging tim...  \n",
       "\n",
       "[732 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Text\", \"Sentiment\"]]\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].str.strip()\n",
    "df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/ywg60v213710yk8053f7h4k00000gn/T/ipykernel_77104/1395013174.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedText</th>\n",
       "      <th>SentimentEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>enjoying beautiful day park!</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>traffic terrible morning.</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>finished amazing workout! ðŸ’ª</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excited upcoming weekend getaway!</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>trying new recipe dinner tonight.</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>collaborating science project received recogni...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>attending surprise birthday party organized fr...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>successfully fundraising school charity initia...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>participating multicultural festival, celebrat...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>organizing virtual talent show challenging tim...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...  Positive   \n",
       "1     Traffic was terrible this morning.           ...  Negative   \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...  Positive   \n",
       "3     Excited about the upcoming weekend getaway!  ...  Positive   \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral   \n",
       "..                                                 ...       ...   \n",
       "727  Collaborating on a science project that receiv...     Happy   \n",
       "728  Attending a surprise birthday party organized ...     Happy   \n",
       "729  Successfully fundraising for a school charity ...     Happy   \n",
       "730  Participating in a multicultural festival, cel...     Happy   \n",
       "731  Organizing a virtual talent show during challe...     Happy   \n",
       "\n",
       "                                         ProcessedText  SentimentEncoded  \n",
       "0                         enjoying beautiful day park!               146  \n",
       "1                            traffic terrible morning.               134  \n",
       "2                          finished amazing workout! ðŸ’ª               146  \n",
       "3                    excited upcoming weekend getaway!               146  \n",
       "4                    trying new recipe dinner tonight.               135  \n",
       "..                                                 ...               ...  \n",
       "727  collaborating science project received recogni...                93  \n",
       "728  attending surprise birthday party organized fr...                93  \n",
       "729  successfully fundraising school charity initia...                93  \n",
       "730  participating multicultural festival, celebrat...                93  \n",
       "731  organizing virtual talent show challenging tim...                93  \n",
       "\n",
       "[732 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedText</th>\n",
       "      <th>SentimentEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>enjoying beautiful day park!</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>traffic terrible morning.</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>finished amazing workout! ðŸ’ª</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excited upcoming weekend getaway!</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>trying new recipe dinner tonight.</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>collaborating science project received recogni...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>attending surprise birthday party organized fr...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>successfully fundraising school charity initia...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>participating multicultural festival, celebrat...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>organizing virtual talent show challenging tim...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...  Positive   \n",
       "1     Traffic was terrible this morning.           ...  Negative   \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...  Positive   \n",
       "3     Excited about the upcoming weekend getaway!  ...  Positive   \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral   \n",
       "..                                                 ...       ...   \n",
       "727  Collaborating on a science project that receiv...     Happy   \n",
       "728  Attending a surprise birthday party organized ...     Happy   \n",
       "729  Successfully fundraising for a school charity ...     Happy   \n",
       "730  Participating in a multicultural festival, cel...     Happy   \n",
       "731  Organizing a virtual talent show during challe...     Happy   \n",
       "\n",
       "                                         ProcessedText  SentimentEncoded  \n",
       "0                         enjoying beautiful day park!               146  \n",
       "1                            traffic terrible morning.               134  \n",
       "2                          finished amazing workout! ðŸ’ª               146  \n",
       "3                    excited upcoming weekend getaway!               146  \n",
       "4                    trying new recipe dinner tonight.               135  \n",
       "..                                                 ...               ...  \n",
       "727  collaborating science project received recogni...                93  \n",
       "728  attending surprise birthday party organized fr...                93  \n",
       "729  successfully fundraising school charity initia...                93  \n",
       "730  participating multicultural festival, celebrat...                93  \n",
       "731  organizing virtual talent show challenging tim...                93  \n",
       "\n",
       "[653 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rare classes (classes with only 1 sample)\n",
    "class_counts = df[\"SentimentEncoded\"].value_counts()\n",
    "rare_classes = class_counts[class_counts < 2].index\n",
    "df = df[~df[\"SentimentEncoded\"].isin(rare_classes)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88,  82,  88,  88,  83,  88,  88,  88,  82,  83,  88,  82,  88,\n",
       "        88,  83,  88,  88,  88,  83,  82,  88,  88,  88,  88,  88,  88,\n",
       "        88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,\n",
       "        88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,  88,\n",
       "         8,  51,  99,  36,  60,  75,  79,   7,  44,   2,   5,  13,  34,\n",
       "       104,   0,   3,   9,  16,  19,  24,  49,  76,  89, 102,   8,  51,\n",
       "        99,  36,  60,  75,  79,   7,  44,   2,   5,  13,  34, 104,   0,\n",
       "         3,   9,  16,  19,  24,  49,  76,  89, 102,   0,  49,  16,  19,\n",
       "        24,  49,  76,  89, 102,  38,  48,  26, 101,  58,  66,  42,  21,\n",
       "       105,  12,  45,  56,  97,  38,  48,  26, 101,  58,  66,  42,  21,\n",
       "       105,  12,  45,  56,  97,  38,  38,  48,  26, 101,  58,  66,  42,\n",
       "        21, 105,  12,  45,  56,  97,  38,  48,  26, 101,  58,  66,  42,\n",
       "        21, 105,  12,  45,  56,  97,  38,  31,  59,  77,  74,  95,  55,\n",
       "        18,  10,  71,  65,  47,  93,  36,  31,  59,  77,  74,  95,  55,\n",
       "        18,  10,  71,  65,  47,  93,  36,  31,  59,  77,  74,  95,  55,\n",
       "        18,  29,  68,  24,  85,  80,  84,   6,   0,  32, 101,  29,  68,\n",
       "        24,  85,  80,  84,   6,   0,  32, 101,  29,  68,  24,  85,  80,\n",
       "        84,   6,   0,  32, 101,  68,  24,  85,  80,  84,   6,   0,  32,\n",
       "       101,  29,  68,  24,  85,  80,  84,   6,   0,  32, 101,  29,  85,\n",
       "       111,  26,  67,  90,  57,  41,  22,  87,  53,  70,  67,  23, 101,\n",
       "        29,   6,  57, 111,  22,  90,  67,  87,  70,  26,  41,  53,  57,\n",
       "        23,  67,  87,  70,  67,  22,  90,  67,  87,  70,  26,  41,  53,\n",
       "        57,  23,  67,  87,  70,  67,  22,  90,  67,  87,  31,  17,  77,\n",
       "       110,  52,  11,  86,  73,  33,  54,  46,  37,  54,  77,  52,  17,\n",
       "        86,  73,  33,  54,  46,  37,  31,  17,  77, 110,  52,  11,  86,\n",
       "        73,  33,  54,  46,  37,  54,  77,  13,  32,  84, 106,  19,  86,\n",
       "        49,  58,  29,   2,  69,  58,  42,   7,  25,  49, 100, 101,   9,\n",
       "        92,  84,  26,  45, 104,   1,  89,  60,  38,  29,  43, 108, 106,\n",
       "        62,  28,  91, 109,  94,  69,  27,  58,   4,  48,  13,  43, 108,\n",
       "        92,  62,  91, 109,  94,  69,  27,  58,  48,  13,  29,  49,  26,\n",
       "        96,  84,  58, 101,  64,  77,  59,  31,  15,  93,  59,  72,  35,\n",
       "        31,  80,  17, 103,  30,  93,  59,  77,  17,  30,  64,  59,  15,\n",
       "        96,  72,  93,  31, 103,  30,  59,  15,  64,  31,  78,  84,  59,\n",
       "        15,  31,  30,  93,  78,  15,  75,  60,  26,  45,  58,   1,  75,\n",
       "        79,  75,  49, 104,  29,  58,  69,  60, 106,  92,  43,  49,  75,\n",
       "        50,  13, 101,  13,  49,  20, 101, 109,  13,   1,  50,   4, 107,\n",
       "        40,  43,  84, 101,  75,  66,  45,  28,  40,  89,  48,  49,  84,\n",
       "        89,  58,  69,  25, 100,  75,  69,  49,   2,  20,  13, 106,  32,\n",
       "        35,  55,  92,  31, 107,   4,  28,  62,  49,  18,  75,  55, 104,\n",
       "        68, 100,  36, 104,  75,  49,  75,  49,  55,  75,  89,  49,  75,\n",
       "        75,  49,  29,  29,  26,  58,  29,  26,  29,  75,  75,  26,  75,\n",
       "        75,  26,  26,  26,  75,  58,  29,  26,  75,  75,  58,  49,  75,\n",
       "        75,  26,  75,  75,  75,  26,  75,  49,  58,  49,  75,  75,  75,\n",
       "        58,  29,  75,  58,  49,  49,  49,  75,  49,  88,  39,  49,  75,\n",
       "        75,  75,  49,  49,  75,  49,  81,  49,  39,  39,  39,  75,  49,\n",
       "        81,  49,  75,  39,  39,  75,  49,  75,  49,  75,  39,  49,  75,\n",
       "        49,  75,  49,  75,  39,  49,  98,  63,  14,  98,  63,  14,  98,\n",
       "        98,  63,  14,  98,  63,  14,  98,  98,  63,  14,  98,  63,  14,\n",
       "        98,  83,  83,  83,  83,  83,  83,  83,  83,  83,  83,  83,  83,\n",
       "        83,  83,  61,  61,  61,  61,  61,  61,  61,  61,  61,  61,  61,\n",
       "        61,  61,  61])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset\n",
    "X = df[\"ProcessedText\"]\n",
    "y = label_encoder.fit_transform(df[\"Sentiment\"])  # Re-encode after removing rare classes\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation / Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(653, 6248))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to numerical features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=20000)  # Increase features for better representation\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3588, 6248)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SMOTE to balance classes\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(random_state=42, k_neighbors=1)\n",
    "X_tfidf, y = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Split after SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the PyTorch MLP model\n",
    "class SentimentMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SentimentMLP, self).__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)                         #LATER\n",
    "        )\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Third hidden layer\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(set(y_train))\n",
    "model = SentimentMLP(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients because we want each batch to function independently of the other\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()             #Compute the gradients wrt the loss above\n",
    "        optimizer.step()            #Update the model parameters based on the above computed gradients\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels for classification report\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 4.7159, Train Acc: 0.0071, Test Loss: 4.7112, Test Acc: 0.0113\n",
      "Epoch 2/30, Train Loss: 4.7081, Train Acc: 0.0082, Test Loss: 4.7037, Test Acc: 0.0102\n",
      "Epoch 3/30, Train Loss: 4.7009, Train Acc: 0.0102, Test Loss: 4.6969, Test Acc: 0.0102\n",
      "Epoch 4/30, Train Loss: 4.6943, Train Acc: 0.0099, Test Loss: 4.6906, Test Acc: 0.0102\n",
      "Epoch 5/30, Train Loss: 4.6879, Train Acc: 0.0102, Test Loss: 4.6840, Test Acc: 0.0102\n",
      "Epoch 6/30, Train Loss: 4.6814, Train Acc: 0.0085, Test Loss: 4.6775, Test Acc: 0.0102\n",
      "Epoch 7/30, Train Loss: 4.6747, Train Acc: 0.0099, Test Loss: 4.6710, Test Acc: 0.0102\n",
      "Epoch 8/30, Train Loss: 4.6682, Train Acc: 0.0108, Test Loss: 4.6644, Test Acc: 0.0102\n",
      "Epoch 9/30, Train Loss: 4.6618, Train Acc: 0.0077, Test Loss: 4.6578, Test Acc: 0.0102\n",
      "Epoch 10/30, Train Loss: 4.6552, Train Acc: 0.0116, Test Loss: 4.6515, Test Acc: 0.0125\n",
      "Epoch 11/30, Train Loss: 4.6493, Train Acc: 0.0102, Test Loss: 4.6451, Test Acc: 0.0102\n",
      "Epoch 12/30, Train Loss: 4.6431, Train Acc: 0.0094, Test Loss: 4.6391, Test Acc: 0.0102\n",
      "Epoch 13/30, Train Loss: 4.6369, Train Acc: 0.0094, Test Loss: 4.6335, Test Acc: 0.0159\n",
      "Epoch 14/30, Train Loss: 4.6323, Train Acc: 0.0096, Test Loss: 4.6286, Test Acc: 0.0102\n",
      "Epoch 15/30, Train Loss: 4.6277, Train Acc: 0.0099, Test Loss: 4.6244, Test Acc: 0.0102\n",
      "Epoch 16/30, Train Loss: 4.6244, Train Acc: 0.0091, Test Loss: 4.6209, Test Acc: 0.0102\n",
      "Epoch 17/30, Train Loss: 4.6212, Train Acc: 0.0091, Test Loss: 4.6181, Test Acc: 0.0102\n",
      "Epoch 18/30, Train Loss: 4.6192, Train Acc: 0.0111, Test Loss: 4.6164, Test Acc: 0.0102\n",
      "Epoch 19/30, Train Loss: 4.6178, Train Acc: 0.0082, Test Loss: 4.6146, Test Acc: 0.0181\n",
      "Epoch 20/30, Train Loss: 4.6160, Train Acc: 0.0116, Test Loss: 4.6131, Test Acc: 0.0193\n",
      "Epoch 21/30, Train Loss: 4.6159, Train Acc: 0.0102, Test Loss: 4.6121, Test Acc: 0.0102\n",
      "Epoch 22/30, Train Loss: 4.6144, Train Acc: 0.0105, Test Loss: 4.6111, Test Acc: 0.0102\n",
      "Epoch 23/30, Train Loss: 4.6148, Train Acc: 0.0094, Test Loss: 4.6104, Test Acc: 0.0102\n",
      "Epoch 24/30, Train Loss: 4.6124, Train Acc: 0.0105, Test Loss: 4.6097, Test Acc: 0.0102\n",
      "Epoch 25/30, Train Loss: 4.6123, Train Acc: 0.0096, Test Loss: 4.6089, Test Acc: 0.0102\n",
      "Epoch 26/30, Train Loss: 4.6121, Train Acc: 0.0094, Test Loss: 4.6083, Test Acc: 0.0215\n",
      "Epoch 27/30, Train Loss: 4.6110, Train Acc: 0.0128, Test Loss: 4.6079, Test Acc: 0.0102\n",
      "Epoch 28/30, Train Loss: 4.6111, Train Acc: 0.0094, Test Loss: 4.6075, Test Acc: 0.0102\n",
      "Epoch 29/30, Train Loss: 4.6100, Train Acc: 0.0091, Test Loss: 4.6068, Test Acc: 0.0102\n",
      "Epoch 30/30, Train Loss: 4.6101, Train Acc: 0.0108, Test Loss: 4.6065, Test Acc: 0.0102\n",
      "PyTorch MLP Accuracy: 0.0215\n",
      "PyTorch Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         9\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.00      0.00      0.00         9\n",
      "          11       0.00      0.00      0.00         9\n",
      "          12       0.00      0.00      0.00         9\n",
      "          13       0.00      0.00      0.00         9\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00         9\n",
      "          18       0.03      1.00      0.06         9\n",
      "          19       0.00      0.00      0.00         9\n",
      "          21       0.00      0.00      0.00         9\n",
      "          22       0.00      0.00      0.00         9\n",
      "          23       0.00      0.00      0.00         9\n",
      "          24       1.00      0.11      0.20         9\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.00      0.00      0.00         9\n",
      "          27       0.00      0.00      0.00         9\n",
      "          28       0.00      0.00      0.00         9\n",
      "          29       0.00      0.00      0.00         9\n",
      "          30       0.00      0.00      0.00         9\n",
      "          31       0.00      0.00      0.00         9\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00         9\n",
      "          34       0.00      0.00      0.00         9\n",
      "          35       0.00      0.00      0.00         9\n",
      "          36       0.00      0.00      0.00         9\n",
      "          37       0.00      0.00      0.00         9\n",
      "          38       0.00      0.00      0.00         9\n",
      "          39       0.00      0.00      0.00         9\n",
      "          40       0.00      0.00      0.00         9\n",
      "          41       0.00      0.00      0.00         9\n",
      "          42       0.00      0.00      0.00         9\n",
      "          43       0.00      0.00      0.00         9\n",
      "          44       0.02      1.00      0.03         9\n",
      "          45       0.00      0.00      0.00         9\n",
      "          46       0.00      0.00      0.00         9\n",
      "          47       0.00      0.00      0.00         9\n",
      "          48       0.00      0.00      0.00         9\n",
      "          49       0.00      0.00      0.00         9\n",
      "          51       0.00      0.00      0.00         9\n",
      "          52       0.00      0.00      0.00         9\n",
      "          53       0.00      0.00      0.00         9\n",
      "          54       0.00      0.00      0.00         9\n",
      "          55       0.00      0.00      0.00         9\n",
      "          56       0.00      0.00      0.00         9\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.00      0.00      0.00         9\n",
      "          59       0.00      0.00      0.00         9\n",
      "          60       0.00      0.00      0.00         9\n",
      "          61       0.00      0.00      0.00         9\n",
      "          62       0.00      0.00      0.00         9\n",
      "          63       0.00      0.00      0.00         9\n",
      "          64       0.00      0.00      0.00         9\n",
      "          65       0.00      0.00      0.00         9\n",
      "          66       0.00      0.00      0.00         9\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       0.00      0.00      0.00         9\n",
      "          69       0.00      0.00      0.00         9\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00         9\n",
      "          73       0.00      0.00      0.00         9\n",
      "          74       0.00      0.00      0.00         9\n",
      "          75       0.00      0.00      0.00         9\n",
      "          76       0.00      0.00      0.00         9\n",
      "          77       0.00      0.00      0.00         9\n",
      "          79       0.00      0.00      0.00         9\n",
      "          80       0.00      0.00      0.00         9\n",
      "          82       0.00      0.00      0.00         1\n",
      "          83       0.00      0.00      0.00         9\n",
      "          84       0.00      0.00      0.00         9\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.00      0.00      0.00         9\n",
      "          87       0.00      0.00      0.00         9\n",
      "          88       0.00      0.00      0.00         9\n",
      "          89       0.00      0.00      0.00         9\n",
      "          90       0.00      0.00      0.00         9\n",
      "          91       0.00      0.00      0.00         9\n",
      "          92       0.00      0.00      0.00         1\n",
      "          93       0.00      0.00      0.00         9\n",
      "          94       0.00      0.00      0.00         9\n",
      "          95       0.00      0.00      0.00         9\n",
      "          96       0.00      0.00      0.00         9\n",
      "          97       0.00      0.00      0.00         9\n",
      "          98       0.00      0.00      0.00         9\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       0.00      0.00      0.00         9\n",
      "         101       0.00      0.00      0.00         9\n",
      "         102       0.00      0.00      0.00         9\n",
      "         104       0.00      0.00      0.00         9\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00         9\n",
      "         108       0.00      0.00      0.00         9\n",
      "         109       0.00      0.00      0.00         9\n",
      "         110       0.00      0.00      0.00         9\n",
      "         111       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.02       882\n",
      "   macro avg       0.01      0.02      0.00       882\n",
      "weighted avg       0.01      0.02      0.00       882\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the PyTorch model\n",
    "epochs = 30\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'best_sentiment_model.pth')\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_sentiment_model.pth'))\n",
    "_, pytorch_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"PyTorch MLP Accuracy: {pytorch_acc:.4f}\")\n",
    "print(\"PyTorch Classification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
