{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings (100d)...\n",
      "Loaded 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "import os\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Use smaller dimension for faster processing, options: 50, 100, 200, 300\n",
    "glove_dim = 100\n",
    "print(f\"Loading GloVe embeddings ({glove_dim}d)...\")\n",
    "# Create cache directory if it doesn't exist\n",
    "os.makedirs('.vector_cache', exist_ok=True)\n",
    "# Load GloVe vectors\n",
    "glove = vocab.GloVe(name='6B', dim=glove_dim, cache='.vector_cache')\n",
    "print(f\"Loaded {len(glove.stoi)} word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>732</td>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#ScienceFairWinner #HighSchoolScience</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>733</td>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#SurpriseCelebration #HighSchoolFriendship</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>734</td>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#CommunityGiving #HighSchoolPhilanthropy</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>735</td>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CulturalCelebration #HighSchoolUnity</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>732</td>\n",
       "      <td>736</td>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#VirtualEntertainment #HighSchoolPositivity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0               0           0   \n",
       "1               1           1   \n",
       "2               2           2   \n",
       "3               3           3   \n",
       "4               4           4   \n",
       "..            ...         ...   \n",
       "727           728         732   \n",
       "728           729         733   \n",
       "729           730         734   \n",
       "730           731         735   \n",
       "731           732         736   \n",
       "\n",
       "                                                  Text    Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1     Traffic was terrible this morning.           ...   Negative     \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3     Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "..                                                 ...          ...   \n",
       "727  Collaborating on a science project that receiv...       Happy    \n",
       "728  Attending a surprise birthday party organized ...       Happy    \n",
       "729  Successfully fundraising for a school charity ...       Happy    \n",
       "730  Participating in a multicultural festival, cel...       Happy    \n",
       "731  Organizing a virtual talent show during challe...       Happy    \n",
       "\n",
       "               Timestamp                                   User     Platform  \\\n",
       "0    2023-01-15 12:30:00                          User123          Twitter     \n",
       "1    2023-01-15 08:45:00                          CommuterX        Twitter     \n",
       "2    2023-01-15 15:45:00                          FitnessFan      Instagram    \n",
       "3    2023-01-15 18:20:00                          AdventureX       Facebook    \n",
       "4    2023-01-15 19:55:00                          ChefCook        Instagram    \n",
       "..                   ...                                    ...          ...   \n",
       "727  2017-08-18 18:20:00       ScienceProjectSuccessHighSchool     Facebook    \n",
       "728  2018-06-22 14:15:00            BirthdayPartyJoyHighSchool    Instagram    \n",
       "729  2019-04-05 17:30:00   CharityFundraisingTriumphHighSchool      Twitter    \n",
       "730  2020-02-29 20:45:00    MulticulturalFestivalJoyHighSchool     Facebook    \n",
       "731  2020-11-15 15:15:00    VirtualTalentShowSuccessHighSchool    Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  \\\n",
       "0        #Nature #Park                                  15.0   30.0   \n",
       "1        #Traffic #Morning                               5.0   10.0   \n",
       "2        #Fitness #Workout                              20.0   40.0   \n",
       "3        #Travel #Adventure                              8.0   15.0   \n",
       "4        #Cooking #Food                                 12.0   25.0   \n",
       "..                                             ...       ...    ...   \n",
       "727         #ScienceFairWinner #HighSchoolScience       20.0   39.0   \n",
       "728    #SurpriseCelebration #HighSchoolFriendship       25.0   48.0   \n",
       "729      #CommunityGiving #HighSchoolPhilanthropy       22.0   42.0   \n",
       "730         #CulturalCelebration #HighSchoolUnity       21.0   43.0   \n",
       "731   #VirtualEntertainment #HighSchoolPositivity       24.0   47.0   \n",
       "\n",
       "          Country  Year  Month  Day  Hour  \n",
       "0       USA        2023      1   15    12  \n",
       "1       Canada     2023      1   15     8  \n",
       "2     USA          2023      1   15    15  \n",
       "3       UK         2023      1   15    18  \n",
       "4      Australia   2023      1   15    19  \n",
       "..            ...   ...    ...  ...   ...  \n",
       "727            UK  2017      8   18    18  \n",
       "728           USA  2018      6   22    14  \n",
       "729        Canada  2019      4    5    17  \n",
       "730            UK  2020      2   29    20  \n",
       "731           USA  2020     11   15    15  \n",
       "\n",
       "[732 rows x 15 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"./datasets/sentimentdataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Load dataset and define preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function with improved cleaning\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    # More comprehensive cleaning\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)  # Keep hashtag content without # symbol\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    \n",
    "    # More advanced tokenization and lemmatization\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Preprocess text and feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)\n",
    "\n",
    "# Extract hashtags as a feature\n",
    "def extract_hashtags(hashtag_str):\n",
    "    if pd.isna(hashtag_str) or hashtag_str == '':\n",
    "        return []\n",
    "    return hashtag_str.split()\n",
    "\n",
    "df['HashtagsList'] = df['Hashtags'].apply(extract_hashtags)\n",
    "df['HashtagCount'] = df['HashtagsList'].apply(len)\n",
    "\n",
    "# Feature engineering from datetime\n",
    "if 'Timestamp' in df.columns:                                                                                           #LATER\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d-%m-%Y %H:%M', errors='coerce')                         #ERRORS=COERCE MEANS THAT IF CONVERSION FAILS FOR ANY VALUE, IT WILL BE SET TO NAT (NOT A TIME)\n",
    "    \n",
    "    # If timestamp conversion failed, try to use the individual date/time columns\n",
    "    if df['Timestamp'].isna().any() and all(col in df.columns for col in ['Year', 'Month', 'Day', 'Hour']):\n",
    "        df['Timestamp'] = pd.to_datetime(\n",
    "            df[['Year', 'Month', 'Day', 'Hour']].fillna(0).astype(int),\n",
    "            format='%Y%m%d%H',\n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    # Extract time features\n",
    "    df['Hour'] = df['Timestamp'].dt.hour\n",
    "    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "    df['Month'] = df['Timestamp'].dt.month\n",
    "    df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)  # 5,6 = weekend\n",
    "    df['TimeOfDay'] = df['Hour'].apply(lambda h: \n",
    "                                     'Morning' if 5 <= h < 12 else\n",
    "                                     'Afternoon' if 12 <= h < 17 else\n",
    "                                     'Evening' if 17 <= h < 21 else\n",
    "                                     'Night')\n",
    "\n",
    "# Process engagement metrics\n",
    "engagement_cols = ['Retweets', 'Likes']\n",
    "for col in engagement_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# Feature: Engagement ratio\n",
    "if all(col in df.columns for col in engagement_cols):                                       #CHECKS IF BOTH 'RETWEETS' AND 'LIKES' COLUMN EXIST IN THE DATAFRAME\n",
    "    df['EngagementTotal'] = df['Retweets'] + df['Likes']                                    \n",
    "    df['RetweetRatio'] = df.apply(lambda row: row['Retweets'] / row['EngagementTotal'] \n",
    "                                if row['EngagementTotal'] > 0 else 0, axis=1)\n",
    "\n",
    "# Feature: Text properties\n",
    "df['TextLength'] = df['ProcessedText'].apply(len)\n",
    "df['WordCount'] = df['ProcessedText'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "df['AvgWordLength'] = df.apply(lambda row: np.mean([len(word) for word in row['ProcessedText'].split()]) \n",
    "                              if row['WordCount'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>...</th>\n",
       "      <th>HashtagsList</th>\n",
       "      <th>HashtagCount</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>TimeOfDay</th>\n",
       "      <th>EngagementTotal</th>\n",
       "      <th>RetweetRatio</th>\n",
       "      <th>TextLength</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>AvgWordLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:00:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#Nature, #Park]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:00:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#Traffic, #Morning]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Morning</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:00:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#Fitness, #Workout]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:00:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#Travel, #Adventure]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Evening</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>7.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:00:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#Cooking, #Food]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Evening</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>732</td>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2017-08-18 18:00:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#ScienceFairWinner #HighSchoolScience</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#ScienceFairWinner, #HighSchoolScience]</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Evening</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "      <td>7.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>733</td>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2018-06-22 14:00:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#SurpriseCelebration #HighSchoolFriendship</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#SurpriseCelebration, #HighSchoolFriendship]</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.342466</td>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>7.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>734</td>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2019-04-05 17:00:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#CommunityGiving #HighSchoolPhilanthropy</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#CommunityGiving, #HighSchoolPhilanthropy]</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Evening</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>7.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>735</td>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-02-29 20:00:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CulturalCelebration #HighSchoolUnity</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#CulturalCelebration, #HighSchoolUnity]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Evening</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>8.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>732</td>\n",
       "      <td>736</td>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-11-15 15:00:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#VirtualEntertainment #HighSchoolPositivity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[#VirtualEntertainment, #HighSchoolPositivity]</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>6.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0               0           0   \n",
       "1               1           1   \n",
       "2               2           2   \n",
       "3               3           3   \n",
       "4               4           4   \n",
       "..            ...         ...   \n",
       "727           728         732   \n",
       "728           729         733   \n",
       "729           730         734   \n",
       "730           731         735   \n",
       "731           732         736   \n",
       "\n",
       "                                                  Text    Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1     Traffic was terrible this morning.           ...   Negative     \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3     Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "..                                                 ...          ...   \n",
       "727  Collaborating on a science project that receiv...       Happy    \n",
       "728  Attending a surprise birthday party organized ...       Happy    \n",
       "729  Successfully fundraising for a school charity ...       Happy    \n",
       "730  Participating in a multicultural festival, cel...       Happy    \n",
       "731  Organizing a virtual talent show during challe...       Happy    \n",
       "\n",
       "              Timestamp                                   User     Platform  \\\n",
       "0   2023-01-15 12:00:00                          User123          Twitter     \n",
       "1   2023-01-15 08:00:00                          CommuterX        Twitter     \n",
       "2   2023-01-15 15:00:00                          FitnessFan      Instagram    \n",
       "3   2023-01-15 18:00:00                          AdventureX       Facebook    \n",
       "4   2023-01-15 19:00:00                          ChefCook        Instagram    \n",
       "..                  ...                                    ...          ...   \n",
       "727 2017-08-18 18:00:00       ScienceProjectSuccessHighSchool     Facebook    \n",
       "728 2018-06-22 14:00:00            BirthdayPartyJoyHighSchool    Instagram    \n",
       "729 2019-04-05 17:00:00   CharityFundraisingTriumphHighSchool      Twitter    \n",
       "730 2020-02-29 20:00:00    MulticulturalFestivalJoyHighSchool     Facebook    \n",
       "731 2020-11-15 15:00:00    VirtualTalentShowSuccessHighSchool    Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  ...  \\\n",
       "0        #Nature #Park                                  15.0   30.0  ...   \n",
       "1        #Traffic #Morning                               5.0   10.0  ...   \n",
       "2        #Fitness #Workout                              20.0   40.0  ...   \n",
       "3        #Travel #Adventure                              8.0   15.0  ...   \n",
       "4        #Cooking #Food                                 12.0   25.0  ...   \n",
       "..                                             ...       ...    ...  ...   \n",
       "727         #ScienceFairWinner #HighSchoolScience       20.0   39.0  ...   \n",
       "728    #SurpriseCelebration #HighSchoolFriendship       25.0   48.0  ...   \n",
       "729      #CommunityGiving #HighSchoolPhilanthropy       22.0   42.0  ...   \n",
       "730         #CulturalCelebration #HighSchoolUnity       21.0   43.0  ...   \n",
       "731   #VirtualEntertainment #HighSchoolPositivity       24.0   47.0  ...   \n",
       "\n",
       "                                       HashtagsList  HashtagCount  DayOfWeek  \\\n",
       "0                                  [#Nature, #Park]             2          6   \n",
       "1                              [#Traffic, #Morning]             2          6   \n",
       "2                              [#Fitness, #Workout]             2          6   \n",
       "3                             [#Travel, #Adventure]             2          6   \n",
       "4                                 [#Cooking, #Food]             2          6   \n",
       "..                                              ...           ...        ...   \n",
       "727        [#ScienceFairWinner, #HighSchoolScience]             2          4   \n",
       "728   [#SurpriseCelebration, #HighSchoolFriendship]             2          4   \n",
       "729     [#CommunityGiving, #HighSchoolPhilanthropy]             2          4   \n",
       "730        [#CulturalCelebration, #HighSchoolUnity]             2          5   \n",
       "731  [#VirtualEntertainment, #HighSchoolPositivity]             2          6   \n",
       "\n",
       "     IsWeekend  TimeOfDay EngagementTotal RetweetRatio  TextLength  WordCount  \\\n",
       "0            1  Afternoon            45.0     0.333333          27          4   \n",
       "1            1    Morning            15.0     0.333333          24          3   \n",
       "2            1  Afternoon            60.0     0.333333          24          3   \n",
       "3            1    Evening            23.0     0.347826          32          4   \n",
       "4            1    Evening            37.0     0.324324          32          5   \n",
       "..         ...        ...             ...          ...         ...        ...   \n",
       "727          0    Evening            59.0     0.338983          86         10   \n",
       "728          0  Afternoon            73.0     0.342466          88         11   \n",
       "729          0    Evening            64.0     0.343750          76          9   \n",
       "730          1    Evening            64.0     0.328125          85          9   \n",
       "731          1  Afternoon            71.0     0.338028          77         10   \n",
       "\n",
       "     AvgWordLength  \n",
       "0         6.000000  \n",
       "1         7.333333  \n",
       "2         7.333333  \n",
       "3         7.250000  \n",
       "4         5.600000  \n",
       "..             ...  \n",
       "727       7.700000  \n",
       "728       7.090909  \n",
       "729       7.555556  \n",
       "730       8.555556  \n",
       "731       6.800000  \n",
       "\n",
       "[732 rows x 26 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Exploratory data analysis and sentiment grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (732, 26)\n",
      "Number of unique sentiments: 279\n",
      "Class distribution (top 10): \n",
      "Sentiment\n",
      "Positive        44\n",
      "Joy             42\n",
      "Excitement      32\n",
      "Neutral         14\n",
      "Contentment     14\n",
      "Happy           14\n",
      "Hopeful          9\n",
      "Sad              9\n",
      "Gratitude        9\n",
      "Curiosity        8\n",
      "Name: count, dtype: int64\n",
      "Number of unique sentiments after grouping: 266\n",
      "Classes remaining after filtering: 31\n",
      "Distribution after filtering: \n",
      "GroupedSentiment\n",
      "Positive             116\n",
      " Excitement           32\n",
      "Negative              26\n",
      "Neutral               18\n",
      " Contentment          14\n",
      " Gratitude             9\n",
      " Embarrassed           8\n",
      " Curiosity             8\n",
      " Loneliness            7\n",
      " Despair               6\n",
      " Playful               6\n",
      " Hate                  6\n",
      " Elation               6\n",
      " Curiosity             5\n",
      " Empowerment           5\n",
      "Disgust                5\n",
      " Bitterness            5\n",
      " Contentment           5\n",
      " Serenity              5\n",
      " Inspired              5\n",
      " Gratitude             5\n",
      " Serenity              5\n",
      " Indifference          5\n",
      " Determination         5\n",
      " Acceptance            5\n",
      " Ambivalence           5\n",
      " Nostalgia             5\n",
      " Melancholy            5\n",
      " Enthusiasm            5\n",
      " Confusion             5\n",
      " Numbness              5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print dataset info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique sentiments: {df['Sentiment'].nunique()}\")\n",
    "print(f\"Class distribution (top 10): \\n{df['Sentiment'].value_counts().head(10)}\")\n",
    "\n",
    "# Remove empty text\n",
    "df = df[df['ProcessedText'].str.strip() != '']\n",
    "\n",
    "# Group similar sentiments to reduce class count\n",
    "# This is a key improvement to handle the high number of classes\n",
    "sentiment_groups = {\n",
    "    'Positive': ['Positive', 'Happy', 'Excited', 'Joy', 'Satisfied', 'Grateful', 'Hopeful', \n",
    "                'Optimistic', 'Pleased', 'Cheerful', 'Enthusiastic', 'Delighted', 'Good', 'Great'],\n",
    "    'Negative': ['Negative', 'Sad', 'Angry', 'Frustrated', 'Disappointed', 'Worried', 'Anxious',\n",
    "                'Annoyed', 'Upset', 'Concerned', 'Unhappy', 'Distressed', 'Irritated', 'Bad'],\n",
    "    'Neutral': ['Neutral', 'Indifferent', 'Calm', 'Balanced', 'Objective', 'Normal'],\n",
    "    'Surprised': ['Surprised', 'Shocked', 'Amazed', 'Astonished'],\n",
    "    'Confused': ['Confused', 'Uncertain', 'Puzzled', 'Perplexed'],\n",
    "    'Fear': ['Fear', 'Scared', 'Afraid', 'Terrified', 'Nervous'],\n",
    "    'Disgust': ['Disgust', 'Repulsed', 'Revolted']\n",
    "}\n",
    "\n",
    "# # Create a mapping function\n",
    "# def map_sentiment(sentiment):\n",
    "#     for group, values in sentiment_groups.items():\n",
    "#         if sentiment.lower() in [v.lower() for v in values]:\n",
    "#             return group\n",
    "#     return sentiment  # Keep original if not in any group, changed from 'Other'\n",
    "\n",
    "def map_sentiment(sentiment):\n",
    "    # Add debugging to see what's happening\n",
    "    sentiment_lower = sentiment.lower().strip()\n",
    "    for group, values in sentiment_groups.items():\n",
    "        values_lower = [v.lower().strip() for v in values]\n",
    "        if sentiment_lower in values_lower:\n",
    "            return group\n",
    "    return sentiment  # Keep original if not in any group\n",
    "\n",
    "# Apply mapping to reduce number of classes\n",
    "df['GroupedSentiment'] = df['Sentiment'].apply(map_sentiment)\n",
    "print(f\"Number of unique sentiments after grouping: {df['GroupedSentiment'].nunique()}\")\n",
    "\n",
    "# Validate no duplicates in your sentiment groups\n",
    "all_values = []\n",
    "for values in sentiment_groups.values():\n",
    "    all_values.extend(values)\n",
    "duplicates = [item for item, count in Counter(all_values).items() if count > 1]\n",
    "if duplicates:\n",
    "    print(f\"Warning! Duplicate values in sentiment groups: {duplicates}\")\n",
    "\n",
    "# Filter classes with few samples (optional)\n",
    "min_samples_per_class = 5\n",
    "class_counts = df[\"GroupedSentiment\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "df = df[df[\"GroupedSentiment\"].isin(valid_classes)]\n",
    "print(f\"Classes remaining after filtering: {df['GroupedSentiment'].nunique()}\")\n",
    "print(f\"Distribution after filtering: \\n{df['GroupedSentiment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Label encoding and feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after filtering: 352\n",
      "Number of unique sentiments after filtering: 31\n",
      "Using features:\n",
      "- Text feature: ProcessedText\n",
      "- Categorical features: ['Platform', 'Country', 'TimeOfDay']\n",
      "- Numerical features: ['HashtagCount', 'IsWeekend', 'TextLength', 'WordCount', 'AvgWordLength']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Encode labels AFTER filtering\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"GroupedSentiment\"])\n",
    "print(f\"Number of samples after filtering: {len(df)}\")\n",
    "print(f\"Number of unique sentiments after filtering: {len(np.unique(y))}\")\n",
    "\n",
    "# Define features to use\n",
    "text_feature = 'ProcessedText'\n",
    "categorical_features = ['Platform', 'Country', 'TimeOfDay']\n",
    "numerical_features = ['HashtagCount', 'IsWeekend', 'TextLength', 'WordCount', 'AvgWordLength']\n",
    "\n",
    "# Filter out any features not in the dataframe\n",
    "categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "numerical_features = [f for f in numerical_features if f in df.columns]\n",
    "\n",
    "print(\"Using features:\")\n",
    "print(f\"- Text feature: {text_feature}\")\n",
    "print(f\"- Categorical features: {categorical_features}\")\n",
    "print(f\"- Numerical features: {numerical_features}\")\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "for feat in categorical_features:\n",
    "    df[feat] = df[feat].fillna('unknown')\n",
    "for feat in numerical_features:\n",
    "    df[feat] = df[feat].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Dataset splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stratified split\n",
      "Train set shape: 281 samples\n",
      "Test set shape: 71 samples\n"
     ]
    }
   ],
   "source": [
    "# Split dataset AFTER encoding labels\n",
    "X_text = df[text_feature]\n",
    "X_categorical = df[categorical_features] if categorical_features else pd.DataFrame()\n",
    "X_numerical = df[numerical_features] if numerical_features else pd.DataFrame()\n",
    "\n",
    "# Use stratified split if possible, otherwise use regular split\n",
    "try:\n",
    "    X_text_train, X_text_test, X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "        X_text, X_categorical, X_numerical, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"Using stratified split\")\n",
    "except ValueError:\n",
    "    print(\"Warning: Stratified split not possible. Using random split.\")\n",
    "    X_text_train, X_text_test, X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "        X_text, X_categorical, X_numerical, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Train set shape: {len(X_text_train)} samples\")\n",
    "print(f\"Test set shape: {len(X_text_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Feature processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to GloVe embeddings...\n",
      "Text features shape after GloVe: (281, 100)\n",
      "Categorical features shape: (281, 54)\n",
      "Numerical features shape: (281, 5)\n",
      "Combined features shape: (281, 159)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36447621,  0.3234956 , -0.06897601, ..., -1.21670014,\n",
       "        -1.1583153 , -0.57603418],\n",
       "       [ 0.16918682,  0.20915571,  0.30206886, ..., -0.2066176 ,\n",
       "        -0.47523744,  0.83267706],\n",
       "       [ 0.067458  ,  0.21078275,  0.29921475, ..., -0.00460109,\n",
       "        -0.13369851,  0.36655937],\n",
       "       ...,\n",
       "       [-0.11144333,  0.49449112, -0.11655111, ...,  1.28830457,\n",
       "         0.89091828,  0.970786  ],\n",
       "       [-0.05229475,  0.13861249,  0.32730749, ..., -1.49952325,\n",
       "        -1.49985423, -0.47935792],\n",
       "       [-0.1832045 ,  0.06354636,  0.42283182, ...,  0.76306165,\n",
       "         0.89091828, -0.17175164]], shape=(281, 159))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Process text features with better TF-IDF parameters\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     ngram_range=(1, 2),\n",
    "#     max_features=10000,\n",
    "#     min_df=2,                                                               #HERE DF IN MIN_DF AND MAX_DF STAND FOR \"DOCUMENT FREQUENCY\". MIN_DF = 2 MEANS IGNORE TERMS THAT APPEAR IN FEWER THAN 2 DOCUMENTS\n",
    "#     max_df=0.9,                                                                 #MAX_DF = 0.9 means that ignore words that appear in more than 90% of the documents (NOTEThat: MIN_DF USES INTEGER VALUES WHILE MAX_DF USES FRACTIONAL VALUES )\n",
    "#     sublinear_tf=True  # Apply sublinear tf scaling (logarithmic)           #APPLY LOGARITHMIC SCALING TO TF (TO REDUCE THE IMPACT OF VERY HIGH FREQUENCY TERMS)\n",
    "# )\n",
    "# X_text_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
    "# X_text_test_tfidf = vectorizer.transform(X_text_test)\n",
    "\n",
    "def text_to_glove_embedding(text, glove_vectors, dim=100):\n",
    "    words = text.split()\n",
    "    # Initialize embeddings array\n",
    "    embedding = np.zeros(dim)\n",
    "    count = 0\n",
    "    # Average embeddings of all words in the text\n",
    "    for word in words:\n",
    "        if word in glove_vectors.stoi:\n",
    "            # Get word index and corresponding vector\n",
    "            idx = glove_vectors.stoi[word]\n",
    "            embedding += glove_vectors.vectors[idx].numpy()\n",
    "            count += 1\n",
    "    # Take average, handle empty texts\n",
    "    if count > 0:\n",
    "        embedding /= count\n",
    "    return embedding\n",
    "\n",
    "print(\"Converting text to GloVe embeddings...\")\n",
    "X_text_train_glove = np.array([text_to_glove_embedding(text, glove, glove_dim) \n",
    "                              for text in X_text_train])\n",
    "X_text_test_glove = np.array([text_to_glove_embedding(text, glove, glove_dim)\n",
    "                            for text in X_text_test])\n",
    "\n",
    "print(f\"Text features shape after GloVe: {X_text_train_glove.shape}\")\n",
    "\n",
    "# Process categorical features\n",
    "if not X_categorical.empty:\n",
    "    categorical_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_cat_train_encoded = categorical_encoder.fit_transform(X_cat_train)\n",
    "    X_cat_test_encoded = categorical_encoder.transform(X_cat_test)\n",
    "    print(f\"Categorical features shape: {X_cat_train_encoded.shape}\")\n",
    "else:\n",
    "    X_cat_train_encoded = np.empty((X_text_train.shape[0], 0))\n",
    "    X_cat_test_encoded = np.empty((X_text_test.shape[0], 0))\n",
    "\n",
    "# Process numerical features\n",
    "if not X_numerical.empty:\n",
    "    scaler = StandardScaler()\n",
    "    X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "    X_num_test_scaled = scaler.transform(X_num_test)\n",
    "    print(f\"Numerical features shape: {X_num_train_scaled.shape}\")\n",
    "else:\n",
    "    X_num_train_scaled = np.empty((X_text_train.shape[0], 0))\n",
    "    X_num_test_scaled = np.empty((X_text_test.shape[0], 0))\n",
    "\n",
    "# Combine all features\n",
    "X_train_combined = np.hstack((X_text_train_glove, X_cat_train_encoded, X_num_train_scaled))\n",
    "X_test_combined = np.hstack((X_text_test_glove, X_cat_test_encoded, X_num_test_scaled))\n",
    "print(f\"Combined features shape: {X_train_combined.shape}\")\n",
    "X_train_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: Dataset preparation for PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply class weights to handle imbalanced dataset\n",
    "class_counts = Counter(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_idx: total_samples / (len(class_counts) * count) for class_idx, count in class_counts.items()}           #CLASS_WEIGHTS REFER TO THE IMPORTANCE THAT NEED TO BE GIVEN TO EACH CLASS (TO GIVE HIGHER WEIGHTS TO UNDERREPRESENTED CLASSES, AND LOWER WEIGHTS TO OTHERS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights = torch.FloatTensor([class_weights.get(i, 1.0) for i in range(len(np.unique(y)))]).to(device)                           #CREATE A PYTORCH TENSOR FOR CLASS WEIGHTS\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_combined)\n",
    "X_test_tensor = torch.FloatTensor(X_test_combined)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Increase batch size if classes are reduced\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: Define neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the improved model with better architecture\n",
    "# class ImprovedSentimentClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "#         super(ImprovedSentimentClassifier, self).__init__()\n",
    "        \n",
    "#         # Improved architecture with better capacity control\n",
    "#         self.layer1 = nn.Linear(input_dim, 512)\n",
    "#         self.bn1 = nn.BatchNorm1d(512)\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.layer2 = nn.Linear(512, 256)\n",
    "#         self.bn2 = nn.BatchNorm1d(256)\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.layer3 = nn.Linear(256, 128)\n",
    "#         self.bn3 = nn.BatchNorm1d(128)\n",
    "#         self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.output_layer = nn.Linear(128, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Use ELU activation for better gradient flow\n",
    "#         x = torch.nn.functional.elu(self.layer1(x))\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.dropout1(x)\n",
    "        \n",
    "#         x = torch.nn.functional.elu(self.layer2(x))\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "#         x = torch.nn.functional.elu(self.layer3(x))\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.dropout3(x)\n",
    "        \n",
    "#         return self.output_layer(x)\n",
    "\n",
    "class GloVeSentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super(GloVeSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Adjust architecture for GloVe embeddings\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.elu(self.layer1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.nn.functional.elu(self.layer2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Initialize model and training components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Building model with 159 input features and 31 output classes\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train_combined.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Building model with {input_dim} input features and {num_classes} output classes\")\n",
    "\n",
    "model = GloVeSentimentClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "# Use weighted cross entropy loss to handle imbalanced classes\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Use AdamW optimizer with weight decay for better regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler with cosine annealing for better convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Define training and evaluation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return running_loss / total, correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Training loop with early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 3.4438, Train Acc: 0.0356, Val Loss: 3.3058, Val Acc: 0.1268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with validation accuracy: 0.1268\n",
      "Epoch 2/100, Train Loss: 2.3829, Train Acc: 0.2349, Val Loss: 3.0377, Val Acc: 0.1972\n",
      "Model saved at epoch 2 with validation accuracy: 0.1972\n",
      "Epoch 3/100, Train Loss: 1.8336, Train Acc: 0.3594, Val Loss: 2.6726, Val Acc: 0.2535\n",
      "Model saved at epoch 3 with validation accuracy: 0.2535\n",
      "Epoch 4/100, Train Loss: 1.4336, Train Acc: 0.4448, Val Loss: 2.3041, Val Acc: 0.2817\n",
      "Model saved at epoch 4 with validation accuracy: 0.2817\n",
      "Epoch 5/100, Train Loss: 1.1054, Train Acc: 0.4875, Val Loss: 2.0093, Val Acc: 0.3239\n",
      "Model saved at epoch 5 with validation accuracy: 0.3239\n",
      "Epoch 6/100, Train Loss: 0.8709, Train Acc: 0.5196, Val Loss: 1.7726, Val Acc: 0.3521\n",
      "Model saved at epoch 6 with validation accuracy: 0.3521\n",
      "Epoch 7/100, Train Loss: 0.7310, Train Acc: 0.5623, Val Loss: 1.5779, Val Acc: 0.3944\n",
      "Model saved at epoch 7 with validation accuracy: 0.3944\n",
      "Epoch 8/100, Train Loss: 0.5990, Train Acc: 0.6085, Val Loss: 1.4668, Val Acc: 0.3944\n",
      "Epoch 9/100, Train Loss: 0.4884, Train Acc: 0.6299, Val Loss: 1.3873, Val Acc: 0.4789\n",
      "Model saved at epoch 9 with validation accuracy: 0.4789\n",
      "Epoch 10/100, Train Loss: 0.4032, Train Acc: 0.6584, Val Loss: 1.3418, Val Acc: 0.4366\n",
      "Epoch 11/100, Train Loss: 0.3325, Train Acc: 0.7082, Val Loss: 1.2988, Val Acc: 0.4648\n",
      "Epoch 12/100, Train Loss: 0.3126, Train Acc: 0.7117, Val Loss: 1.2504, Val Acc: 0.5211\n",
      "Model saved at epoch 12 with validation accuracy: 0.5211\n",
      "Epoch 13/100, Train Loss: 0.2513, Train Acc: 0.7367, Val Loss: 1.2187, Val Acc: 0.5211\n",
      "Epoch 14/100, Train Loss: 0.2129, Train Acc: 0.7509, Val Loss: 1.2031, Val Acc: 0.5211\n",
      "Epoch 15/100, Train Loss: 0.1957, Train Acc: 0.7722, Val Loss: 1.2132, Val Acc: 0.5211\n",
      "Epoch 16/100, Train Loss: 0.1743, Train Acc: 0.7972, Val Loss: 1.2349, Val Acc: 0.5775\n",
      "Model saved at epoch 16 with validation accuracy: 0.5775\n",
      "Epoch 17/100, Train Loss: 0.1468, Train Acc: 0.8363, Val Loss: 1.2495, Val Acc: 0.5775\n",
      "Epoch 18/100, Train Loss: 0.1533, Train Acc: 0.8256, Val Loss: 1.2762, Val Acc: 0.5493\n",
      "Epoch 19/100, Train Loss: 0.1260, Train Acc: 0.8434, Val Loss: 1.2790, Val Acc: 0.5775\n",
      "Epoch 20/100, Train Loss: 0.1207, Train Acc: 0.8399, Val Loss: 1.2631, Val Acc: 0.5775\n",
      "Epoch 21/100, Train Loss: 0.1155, Train Acc: 0.8897, Val Loss: 1.2634, Val Acc: 0.5634\n",
      "Epoch 22/100, Train Loss: 0.0952, Train Acc: 0.8861, Val Loss: 1.2749, Val Acc: 0.5775\n",
      "Epoch 23/100, Train Loss: 0.0986, Train Acc: 0.9039, Val Loss: 1.3091, Val Acc: 0.5634\n",
      "Early stopping after 7 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "best_accuracy = 0.0\n",
    "patience = 7\n",
    "no_improvement = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Save best model and check for early stopping\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'accuracy': val_acc,\n",
    "    'label_encoder': label_encoder,\n",
    "    'glove_dim': glove_dim,\n",
    "    'glove_name': '6B',  # Adding the specific GloVe model name\n",
    "    'categorical_encoder': categorical_encoder if not X_categorical.empty else None,\n",
    "    'scaler': scaler if not X_numerical.empty else None,\n",
    "    'categorical_features': categorical_features,\n",
    "    'numerical_features': numerical_features,\n",
    "}, 'best_sentiment_model.pth')\n",
    "            print(f'Model saved at epoch {epoch+1} with validation accuracy: {val_acc:.4f}')\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement == patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement')\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 13: Final model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 16 with validation accuracy: 0.5775\n",
      "\n",
      "Final Test Accuracy: 0.5775\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Acceptance             0.50      1.00      0.67         1\n",
      " Ambivalence            1.00      1.00      1.00         1\n",
      "      Bitterness        1.00      1.00      1.00         1\n",
      " Confusion              1.00      1.00      1.00         1\n",
      "     Contentment        0.50      0.33      0.40         3\n",
      "   Contentment          1.00      0.00      0.00         1\n",
      "       Curiosity        0.33      0.50      0.40         2\n",
      " Curiosity              1.00      1.00      1.00         1\n",
      "         Despair        0.50      1.00      0.67         1\n",
      " Determination          1.00      1.00      1.00         1\n",
      "   Elation              0.33      1.00      0.50         1\n",
      "     Embarrassed        0.00      0.00      0.00         2\n",
      "   Empowerment          1.00      0.00      0.00         1\n",
      "   Enthusiasm           0.33      1.00      0.50         1\n",
      "      Excitement        0.14      0.14      0.14         7\n",
      "       Gratitude        0.25      0.50      0.33         2\n",
      "   Gratitude            1.00      1.00      1.00         1\n",
      "            Hate        1.00      1.00      1.00         1\n",
      " Indifference           1.00      0.00      0.00         1\n",
      "        Inspired        1.00      1.00      1.00         1\n",
      "      Loneliness        0.00      0.00      0.00         1\n",
      " Melancholy             0.00      0.00      0.00         1\n",
      " Nostalgia              1.00      0.00      0.00         1\n",
      " Numbness               1.00      1.00      1.00         1\n",
      "         Playful        0.50      1.00      0.67         1\n",
      "   Serenity             1.00      1.00      1.00         1\n",
      " Serenity               1.00      1.00      1.00         1\n",
      "          Disgust       1.00      1.00      1.00         1\n",
      "         Negative       0.80      0.80      0.80         5\n",
      "          Neutral       0.67      0.50      0.57         4\n",
      "         Positive       0.82      0.61      0.70        23\n",
      "\n",
      "         accuracy                           0.58        71\n",
      "        macro avg       0.70      0.66      0.59        71\n",
      "     weighted avg       0.67      0.58      0.58        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best model for final evaluation\n",
    "try:\n",
    "    # Add these lines to fix the LabelEncoder loading issue\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    torch.serialization.add_safe_globals([LabelEncoder])\n",
    "    \n",
    "    # Now load with weights_only=False to allow loading Python objects\n",
    "    checkpoint = torch.load('best_sentiment_model.pth', weights_only=False)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with validation accuracy: {checkpoint['accuracy']:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Best model file not found. Continuing with current model state.\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_loss, test_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=label_encoder.classes_, zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
