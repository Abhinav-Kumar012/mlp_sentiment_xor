{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'NLP (Python 3.7.12)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n NLP ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./sentimentdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Lemmatization\n",
    "    return text\n",
    "\n",
    "# Clean up dataframe and preprocess text\n",
    "df = df[[\"Text\", \"Sentiment\"]]\n",
    "df = df.dropna()  # Remove any rows with missing values\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].str.strip()\n",
    "df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)\n",
    "\n",
    "# Print initial class distribution\n",
    "print(f\"Original number of samples: {len(df)}\")\n",
    "print(f\"Original number of unique sentiments: {df['Sentiment'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels and print distribution\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "class_counts = df[\"SentimentEncoded\"].value_counts()\n",
    "print(f\"Class distribution before filtering (top 10): \\n{class_counts.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very rare classes (with only 1 sample)\n",
    "rare_classes = class_counts[class_counts < 2].index\n",
    "print(f\"Number of classes with only 1 sample (to be removed): {len(rare_classes)}\")\n",
    "df = df[~df[\"SentimentEncoded\"].isin(rare_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encode labels after filtering to get consecutive class indices\n",
    "X = df[\"ProcessedText\"]\n",
    "y_classes = df[\"Sentiment\"]\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_classes)\n",
    "\n",
    "print(f\"Number of samples after filtering: {len(df)}\")\n",
    "print(f\"Number of unique sentiments after filtering: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution after filtering (sample counts): {np.bincount(y)[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset BEFORE applying SMOTE\n",
    "X_train, X_test, y_train, y_test, classes_train, classes_test = train_test_split(\n",
    "    X, y, y_classes, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set shape: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF features shape: {X_train_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE ONLY to the training data\n",
    "class_counts_train = np.bincount(y_train)\n",
    "print(f\"Training set class distribution before SMOTE (min: {min(class_counts_train)}, max: {max(class_counts_train)})\")\n",
    "\n",
    "try:\n",
    "    # Use regular SMOTE instead of BorderlineSMOTE for better handling of many classes\n",
    "    # Set sampling_strategy to 'auto' to balance all classes to the majority class size\n",
    "    # Use k_neighbors=1 for classes with very few samples\n",
    "    smote = SMOTE(random_state=42, k_neighbors=1, sampling_strategy='not majority')\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "    print(\"SMOTE successfully applied\")\n",
    "    print(f\"Training samples after SMOTE: {X_train_resampled.shape[0]}\")\n",
    "    unique, counts = np.unique(y_train_resampled, return_counts=True)\n",
    "    print(f\"Class distribution after SMOTE (min: {min(counts)}, max: {max(counts)})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying SMOTE: {e}\")\n",
    "    print(\"Continuing without SMOTE\")\n",
    "    X_train_resampled, y_train_resampled = X_train_tfidf, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_resampled.toarray())\n",
    "X_test_tensor = torch.FloatTensor(X_test_tfidf.toarray())\n",
    "y_train_tensor = torch.LongTensor(y_train_resampled)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Use a smaller batch size for many classes\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an improved PyTorch model for many classes\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.5):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # First fully connected layer with batch normalization\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        # Second fully connected layer with batch normalization\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout and activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First layer with batch norm, activation, and dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second layer with batch norm, activation, and dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "num_classes = len(np.unique(y_train_resampled))\n",
    "print(f\"Building model with {input_dim} input features and {num_classes} output classes\")\n",
    "\n",
    "model = SentimentClassifier(input_dim, num_classes, dropout_rate=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / total, correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 30\n",
    "best_accuracy = 0.0\n",
    "patience = 5\n",
    "no_improvement = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduler based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Save best model and check for early stopping\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': val_acc,\n",
    "                'label_encoder': label_encoder\n",
    "            }, 'best_sentiment_model.pth')\n",
    "            print(f'Model saved at epoch {epoch+1} with validation accuracy: {val_acc:.4f}')\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement == patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement')\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for final evaluation\n",
    "try:\n",
    "    checkpoint = torch.load('best_sentiment_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with validation accuracy: {checkpoint['accuracy']:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Best model file not found. Continuing with current model state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_loss, test_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, zero_division=1))\n",
    "\n",
    "# Convert numeric predictions back to original sentiment labels\n",
    "predicted_classes = label_encoder.inverse_transform(all_preds)\n",
    "true_classes = label_encoder.inverse_transform(all_labels)\n",
    "\n",
    "# Print confusion matrix for top classes (if matplotlib is available)\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Get most frequent classes\n",
    "    top_class_indices = np.argsort(np.bincount(all_labels))[-10:]  # Top 10 classes\n",
    "    mask = np.isin(all_labels, top_class_indices)\n",
    "    \n",
    "    if np.sum(mask) > 0:  # If we have any samples from top classes\n",
    "        cm = confusion_matrix(\n",
    "            [true_classes[i] for i in range(len(true_classes)) if mask[i]],\n",
    "            [predicted_classes[i] for i in range(len(predicted_classes)) if mask[i]]\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=[label_encoder.inverse_transform([i])[0] for i in top_class_indices],\n",
    "                   yticklabels=[label_encoder.inverse_transform([i])[0] for i in top_class_indices])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix for Top Classes')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        print(\"Confusion matrix saved to 'confusion_matrix.png'\")\n",
    "except ImportError:\n",
    "    print(\"Matplotlib or seaborn not available for confusion matrix visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment for new text\n",
    "def predict_sentiment(text, model, vectorizer, label_encoder, device):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_tfidf = vectorizer.transform([processed_text]).toarray()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    text_tensor = torch.FloatTensor(text_tfidf).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert to sentiment label\n",
    "    predicted_sentiment = label_encoder.inverse_transform(predicted.cpu().numpy())[0]\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# Example of prediction function usage\n",
    "print(\"\\nExample prediction function:\")\n",
    "print(\"predict_sentiment('I love this product!', model, vectorizer, label_encoder, device)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
