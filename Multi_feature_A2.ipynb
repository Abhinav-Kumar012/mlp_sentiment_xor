{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpgSMwlpeaB6"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from datetime import datetime\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0412MPheaB8",
        "outputId": "c72f05a2-17c4-411c-ca09-8ec4977d9548"
      },
      "outputs": [],
      "source": [
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnaYaFXmgGNq"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"./sentimentdataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5AgKRGN9rCc"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english')) | ENGLISH_STOP_WORDS\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ6ZSNGg9uzB"
      },
      "outputs": [],
      "source": [
        "def extract_nlp_features(text):\n",
        "    # Initialize VADER sentiment analyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Check if text is empty\n",
        "    if not text or pd.isna(text):\n",
        "        return pd.Series({\n",
        "            'text_length': 0,\n",
        "            'word_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'vader_compound': 0,\n",
        "            'vader_neg': 0,\n",
        "            'vader_neu': 0,\n",
        "            'vader_pos': 0\n",
        "        })\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Get sentiment scores\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "    # Extract text statistics\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    text_length = len(text)\n",
        "    avg_word_length = text_length / max(word_count, 1)  # Avoid division by zero\n",
        "\n",
        "    return pd.Series({\n",
        "        'text_length': text_length,\n",
        "        'word_count': word_count,\n",
        "        'avg_word_length': avg_word_length,\n",
        "        'vader_compound': sentiment_scores['compound'],\n",
        "        'vader_neg': sentiment_scores['neg'],\n",
        "        'vader_neu': sentiment_scores['neu'],\n",
        "        'vader_pos': sentiment_scores['pos']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUZFpXsTKouM"
      },
      "outputs": [],
      "source": [
        "def create_time_features(df):\n",
        "    time_cols = ['Year', 'Month', 'Day', 'Hour']\n",
        "    for col in time_cols:\n",
        "        df[col.lower()] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df['is_weekend'] = df['Day'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQdgGRwqKzWh"
      },
      "outputs": [],
      "source": [
        "def create_engagement_features(df):\n",
        "    df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce').fillna(0)\n",
        "    df['Retweets'] = pd.to_numeric(df['Retweets'], errors='coerce').fillna(0)\n",
        "    df['total_engagement'] = df['Likes'] + df['Retweets']\n",
        "    df['engagement_ratio'] = df['Retweets'] / df['Likes'].replace(0, 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPUgjF3ReaB9",
        "outputId": "ab5a0546-aa7b-4836-86a4-6c04074d97da"
      },
      "outputs": [],
      "source": [
        "# Clean up dataframe and preprocess text\n",
        "print(\"Preprocessing data...\")\n",
        "\n",
        "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], errors='ignore')\n",
        "df = df.dropna()\n",
        "\n",
        "# Clean sentiment values\n",
        "df[\"Sentiment\"] = df[\"Sentiment\"].str.strip()\n",
        "\n",
        "# Preprocess text and create NLP features\n",
        "df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)\n",
        "nlp_features = df[\"Text\"].apply(extract_nlp_features)\n",
        "df = pd.concat([df, nlp_features], axis=1)\n",
        "\n",
        "# Create time features\n",
        "df = create_time_features(df)\n",
        "\n",
        "# Create engagement features\n",
        "df = create_engagement_features(df)\n",
        "\n",
        "# Print initial class distribution\n",
        "print(f\"Original number of samples: {len(df)}\")\n",
        "print(f\"Original number of unique sentiments: {df['Sentiment'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqoX6-nteaB-",
        "outputId": "58957a65-1207-4d39-891e-3bcb74057c50"
      },
      "outputs": [],
      "source": [
        "# Encode labels and print distribution\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
        "class_counts = df[\"SentimentEncoded\"].value_counts()\n",
        "print(f\"Class distribution before filtering (top 10): \\n{class_counts.head(10)}\")\n",
        "\n",
        "# Remove very rare classes (with only 1 sample)\n",
        "rare_classes = class_counts[class_counts < 2].index\n",
        "print(f\"Number of classes with only 1 sample (to be removed): {len(rare_classes)}\")\n",
        "df = df[~df[\"SentimentEncoded\"].isin(rare_classes)]\n",
        "\n",
        "# Re-encode labels after filtering to get consecutive class indices\n",
        "X = df[\"ProcessedText\"]\n",
        "y_classes = df[\"Sentiment\"]\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_classes)\n",
        "\n",
        "# Prepare feature sets\n",
        "# 1. Text features\n",
        "print(f\"Number of samples after filtering: {len(df)}\")\n",
        "print(f\"Number of unique sentiments after filtering: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution after filtering (sample counts): {np.bincount(y)[:10]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Reference words for sentiment categories\n",
        "ref_words = {\n",
        "    \"Positive\": \"positive\",\n",
        "    \"Negative\": \"negative\",\n",
        "    \"Neutral\": \"neutral\"\n",
        "}\n",
        "\n",
        "# Generate reference vectors for each category\n",
        "ref_vectors = {category: nlp(word).vector for category, word in ref_words.items()}\n",
        "\n",
        "def assign_sentiment_category(sentiment):\n",
        "    word_vector = nlp(sentiment).vector.reshape(1, -1)\n",
        "    \n",
        "    similarities = {}\n",
        "    for category, ref_vec in ref_vectors.items():\n",
        "        ref_vec = ref_vec.reshape(1, -1)\n",
        "        sim = cosine_similarity(word_vector, ref_vec)[0][0]\n",
        "        similarities[category] = sim\n",
        "\n",
        "    return max(similarities, key=similarities.get)\n",
        "\n",
        "# Apply the function to the Sentiment column\n",
        "df['Sentiment'] = df['Sentiment'].apply(assign_sentiment_category)\n",
        "\n",
        "# Print the value counts of the new sentiment categories\n",
        "print(df['Sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh3YpIYDeaB_",
        "outputId": "7d21c2c3-3a6e-4634-89ff-7e5fbb2b3018"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test, classes_train, classes_test = train_test_split(\n",
        "    X, y, y_classes, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train set shape: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set shape: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK7-4KIceaB_",
        "outputId": "10cbe02b-8872-4a57-ce6c-1e5013cd0116"
      },
      "outputs": [],
      "source": [
        "# Convert text to numerical features\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF features shape: {X_train_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTi_yCzqeaCA",
        "outputId": "fb457a4d-ff15-43a4-e2e7-50e80711b8e8"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE ONLY to the training data\n",
        "class_counts_train = np.bincount(y_train)\n",
        "print(f\"Training set class distribution before SMOTE (min: {min(class_counts_train)}, max: {max(class_counts_train)})\")\n",
        "\n",
        "try:\n",
        "    # Use regular SMOTE instead of BorderlineSMOTE for better handling of many classes\n",
        "    # Set sampling_strategy to 'auto' to balance all classes to the majority class size\n",
        "    # Use k_neighbors=1 for classes with very few samples\n",
        "    smote = SMOTE(random_state=42, k_neighbors=1, sampling_strategy='not majority')\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "    print(\"SMOTE successfully applied\")\n",
        "    print(f\"Training samples after SMOTE: {X_train_resampled.shape[0]}\")\n",
        "    unique, counts = np.unique(y_train_resampled, return_counts=True)\n",
        "    print(f\"Class distribution after SMOTE (min: {min(counts)}, max: {max(counts)})\")\n",
        "except Exception as e:\n",
        "    print(f\"Error applying SMOTE: {e}\")\n",
        "    print(\"Continuing without SMOTE\")\n",
        "    X_train_resampled, y_train_resampled = X_train_tfidf, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wct_gEeqeaCB"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_resampled.toarray())\n",
        "X_test_tensor = torch.FloatTensor(X_test_tfidf.toarray())\n",
        "y_train_tensor = torch.LongTensor(y_train_resampled)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Use a smaller batch size for many classes\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUqvzZETeaCB"
      },
      "outputs": [],
      "source": [
        "# Define an improved PyTorch model for many classes\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # First fully connected layer with batch normalization\n",
        "        self.fc1 = nn.Linear(input_dim, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # Second fully connected layer with batch normalization\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Dropout and activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with batch norm, activation, and dropout\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second layer with batch norm, activation, and dropout\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZY3zJydeaCC",
        "outputId": "5fd29b12-b785-4c0b-f733-bafacd894caa"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "num_classes = len(np.unique(y_train_resampled))\n",
        "print(f\"Building model with {input_dim} input features and {num_classes} output classes\")\n",
        "\n",
        "model = SentimentClassifier(input_dim, num_classes, dropout_rate=0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARJvnlaQcMqb"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return running_loss / total, correct / total, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bIaYgYqBeaCC",
        "outputId": "583f2c79-0d97-4367-e934-d08fe43e9bc1"
      },
      "outputs": [],
      "source": [
        "# Training loop with early stopping\n",
        "num_epochs = 300\n",
        "best_accuracy = 0.0\n",
        "patience = 50\n",
        "no_improvement = 0\n",
        "\n",
        "try:\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        # Learning rate scheduler based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Save best model and check for early stopping\n",
        "        if val_acc > best_accuracy:\n",
        "            best_accuracy = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'accuracy': val_acc,\n",
        "                'label_encoder': label_encoder\n",
        "            }, 'best_sentiment_model.pth')\n",
        "            print(f'Model saved at epoch {epoch+1} with validation accuracy: {val_acc:.4f}')\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement == patience:\n",
        "                print(f'Early stopping after {patience} epochs without improvement')\n",
        "                break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted!\")\n",
        "\n",
        "# Load the best model for final evaluation\n",
        "try:\n",
        "    # Add this line to register your LabelEncoder class as safe for loading:\n",
        "    torch.serialization.add_safe_globals([LabelEncoder, np.core.multiarray._reconstruct])\n",
        "    checkpoint = torch.load('best_sentiment_model.pth', weights_only=False) # Explicitly set weights_only=False\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with validation accuracy: {checkpoint['accuracy']:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Best model file not found. Continuing with current model state.\")\n",
        "\n",
        "# Final evaluation on test set\n",
        "test_loss, test_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, zero_division=1))\n",
        "\n",
        "# Convert numeric predictions back to original sentiment labels\n",
        "predicted_classes = label_encoder.inverse_transform(all_preds)\n",
        "true_classes = label_encoder.inverse_transform(all_labels)\n",
        "\n",
        "# Print confusion matrix for top classes (if matplotlib is available)\n",
        "try:\n",
        "\n",
        "    # Get most frequent classes\n",
        "    top_class_indices = np.argsort(np.bincount(all_labels))[-10:]  # Top 10 classes\n",
        "    mask = np.isin(all_labels, top_class_indices)\n",
        "\n",
        "    if np.sum(mask) > 0:  # If we have any samples from top classes\n",
        "        cm = confusion_matrix(\n",
        "            [true_classes[i] for i in range(len(true_classes)) if mask[i]],\n",
        "            [predicted_classes[i] for i in range(len(predicted_classes)) if mask[i]]\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=[label_encoder.inverse_transform([i])[0] for i in top_class_indices],\n",
        "                   yticklabels=[label_encoder.inverse_transform([i])[0] for i in top_class_indices])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix for Top Classes')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        print(\"Confusion matrix saved to 'confusion_matrix.png'\")\n",
        "except ImportError:\n",
        "    print(\"Matplotlib or seaborn not available for confusion matrix visualization\")\n",
        "\n",
        "# Function to predict sentiment for new text\n",
        "def predict_sentiment(text, model, vectorizer, label_encoder, device):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Vectorize\n",
        "    text_tfidf = vectorizer.transform([processed_text]).toarray()\n",
        "\n",
        "    # Convert to tensor\n",
        "    text_tensor = torch.FloatTensor(text_tfidf).to(device)\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(text_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Convert to sentiment label\n",
        "    predicted_sentiment = label_encoder.inverse_transform(predicted.cpu().numpy())[0]\n",
        "\n",
        "    return predicted_sentiment\n",
        "\n",
        "# Example of prediction function usage\n",
        "print(\"\\nExample prediction function:\")\n",
        "print(\"predict_sentiment('I love this product!', model, vectorizer, label_encoder, device)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
